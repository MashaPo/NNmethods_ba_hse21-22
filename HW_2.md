***Домашнее задание №2 (построение собственных сетей CNN + FFN)***

В этом домашнем задании вы построите нейронную сеть с использованием сверточных и линейных слоев  для задачи классификации твитов. 
С корпусом мы работали на паре про перцептрон.
Посмотреть и скачать корпус можно из источника http://study.mokoron.com, (либо сразу по прямым ссылкам в ноутбуке)

```
!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv
!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv 
```

Решите задачу , используя две следующие архитектуры, для каждой посчитайте accuracy, precision, recall:

I.  CNN на уровне слов:
модель берет слова, пропускает их через Embedding слой. По эмбеддингам проходит CNN c фильтрами с разным окном, полученные результаты конкатенируются друг с другом по глубине, по результату конкатенации еще один сверточный слой, далее max pooling over time, на выходе линейный слой + сигмоида, функция потерь  BCELoss. (модель аналогична тому, что мы делали на паре по сверткам, но на уровне слов, а не символов)



II. комбинация эмбеддингов и символьных признаков:
У этой модели два входа, один для эмбеддингов слов (предобученных или обучаемых), из них берем max или  mean, делаем вектор для предложения, поверх линейный слой - получаем вектор X.
 Другой вход сети для символьного представления слов (это обучаемый Embedding  слой, он будет брать на вход batch_size x symbols_len и сопоставлять каждому  символу в каждом слове один эмбеддинг). Следующий слой сверточный, примените фильтры разных размеров. Результаты  агрегируются с помощью  max pooling over time и полученные векторы конкатенируются с вектором X.

 Далее линейный слой + сигмоида, функция потерь  BCELoss.

***Баллы:*** 

Embedding слой обучается внутри первой модели -- 1 балл 

подгружаются обученные эмбеддинги для русского языка или
fasttext эмбеддинги обучаются на всем корпусе с нуля  и далее подгружаются в модель -- 1 балл 

Первая модель правильно построена и обучается -- 1 балл 

Правильно подготовленный класс Dataset  для второй архитектуры (теперь вам нужно два словаря (word2id и symbol2id) и две матрицы для кодирования предложения: одна размера batch_size x seq_len,  другая  batch_size x sybmols_len ) -- 2 балла 

Вторая модель правильно построена и обучается -- 2 балла 

Для первой модели показано, что качество можно улучшить засчет работы с гиперпараметрами и/или доведения архитектуры (помогло изменение количества/размера фильтров, размера слоя/эмбеддингов, добавили Dropout и спаслись от переобучения, добавили слоев и т.д.) -- 1 балл 

Для второй модели показано, что качество можно улучшить засчет работы с гиперпараметрами и/или доведения архитектуры (помогло изменение количества/размера фильтров, размера слоя/эмбеддингов, добавили Dropout и спаслись от переобучения, добавили слоев и т.д.) -- 1 балл 

Сделана попытка анализа предсказаний, на примерах из тестовой выборки показано, какие случаи даются легко, а какие хуже - 1 балл 




