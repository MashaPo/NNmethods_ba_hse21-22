{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "9_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyh0knhqJx8u"
      },
      "source": [
        "понятное [видео](https://www.youtube.com/watch?v=bNb2fEVKeEo) со стенфордского курса, из [материалов](https://cs231n.github.io/convolutional-networks) по которому взяты иллюстрации.\n",
        "\n",
        "Датасет из [курса](https://github.com/DanAnastasyev/DeepNLP-Course/blob/master/Week%2004/Week_04_Convolutional_Neural_Networks.ipynb) Даниила Анастасьева.\n",
        "\n",
        "Использовались [материалы](https://github.com/mannefedov/hse_ml_m1/blob/master/7_cnn/cnn.ipynb) из курса Михаила Нефедова."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdbHKxg6J8Q3"
      },
      "source": [
        "# Сверточный слой"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G20z18IaJ6Y5"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Желтое - фильтр (= filter,kernel).\n",
        "\n",
        "Зеленое - входные данные, например, изображение.\n",
        "\n",
        "Розовое - карта активации (activation map).\n",
        "\n",
        "Каждый элемент в розовой матрице - результат поэлементного умножения фильтра на числа из области на входных данных.\n",
        "Обучаемые параметры - элементы фильтра.\n",
        "\n",
        "![Conv](https://image.ibb.co/e6t8ZK/Convolution.gif)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXNixkx2KHsm"
      },
      "source": [
        "Чтобы не терять размер матрицы используется паддинг.\n",
        "\n",
        "![padding](https://3deep.ru/wp-content/uploads/2020/01/keras_conv2d_padding.gif)\n",
        "\n",
        "from https://3deep.ru/machinelearning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp44YeUKKSem"
      },
      "source": [
        "# Pooling слой  (не обучается)\n",
        "\n",
        "![Pool](https://cs231n.github.io/assets/cnn/pool.jpeg)\n",
        "![maxpool](https://cs231n.github.io/assets/cnn/maxpool.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VqCU9o5KUmr"
      },
      "source": [
        "# Свертки для текстов устроены немного по-другому. В них на одну размерность меньше.\n",
        "\n",
        "![text-convs](https://image.ibb.co/bC3Xun/2018_03_27_01_24_39.png)\n",
        "\n",
        "From [Character-Aware Neural Language Models](https://arxiv.org/pdf/1508.06615.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXx-iELMKUjt"
      },
      "source": [
        "# CNN для обработки текстов\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_LxOOJUyn-E",
        "outputId": "6ad4b0f7-9eaf-4e43-e4db-103ce1603684"
      },
      "source": [
        "!pip install torchmetrics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NovB599mOf9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f83601f-e2c1-4d76-e7da-294805cb15eb"
      },
      "source": [
        "!pip install ipdb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipdb in /usr/local/lib/python3.7/dist-packages (0.13.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n",
            "Requirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: ipython>=7.17.0 in /usr/local/lib/python3.7/dist-packages (from ipdb) (7.29.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (3.0.22)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgiT9Xow1sd5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "import ipdb"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfxNXovr1sd6"
      },
      "source": [
        "### Находим фамилии среди слов русского языка\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZtLedfF00Ih"
      },
      "source": [
        "### Скачивание и подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEBb-YaTR1os",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d4e7170-3a62-4000-8b04-64e79c0179bf"
      },
      "source": [
        "!wget -O surnames.txt  \"https://drive.google.com/uc?export=download&id=1z7avv1JiI30V4cmHJGFIfDEs9iE4SHs5\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-22 20:48:59--  https://drive.google.com/uc?export=download&id=1z7avv1JiI30V4cmHJGFIfDEs9iE4SHs5\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.100, 172.217.214.139, 172.217.214.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-6k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/a18gb4m44nsgsn80out30cq1hh489q7f/1637614125000/14571764687846983170/*/1z7avv1JiI30V4cmHJGFIfDEs9iE4SHs5?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-11-22 20:49:00--  https://doc-08-6k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/a18gb4m44nsgsn80out30cq1hh489q7f/1637614125000/14571764687846983170/*/1z7avv1JiI30V4cmHJGFIfDEs9iE4SHs5?e=download\n",
            "Resolving doc-08-6k-docs.googleusercontent.com (doc-08-6k-docs.googleusercontent.com)... 108.177.121.132, 2607:f8b0:4001:c19::84\n",
            "Connecting to doc-08-6k-docs.googleusercontent.com (doc-08-6k-docs.googleusercontent.com)|108.177.121.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1761222 (1.7M) [text/plain]\n",
            "Saving to: ‘surnames.txt’\n",
            "\n",
            "surnames.txt        100%[===================>]   1.68M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-11-22 20:49:00 (121 MB/s) - ‘surnames.txt’ saved [1761222/1761222]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17_AP7LhLPnv"
      },
      "source": [
        "data = pd.read_csv('surnames.txt', encoding='utf-8', sep='\\t', header=None,  names=['word','label'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JYJtXCaERbb"
      },
      "source": [
        "train_data, val_data = train_test_split(data, test_size=0.2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfA7yoOFA4cm",
        "outputId": "ed113ddd-a4c9-4019-cef1-a60898ecd691"
      },
      "source": [
        "train_data[train_data.label == 1].shape[0] /train_data.shape[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12144121208357912"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "45ebLSllFFAW",
        "outputId": "dc094ba1-a780-4e5c-e784-3101463299b6"
      },
      "source": [
        "train_data.head(50)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9035</th>\n",
              "      <td>видеокассета</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9032</th>\n",
              "      <td>видеокартах</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66634</th>\n",
              "      <td>стужей</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32053</th>\n",
              "      <td>ЛИДИЯ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24413</th>\n",
              "      <td>исправлением</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44867</th>\n",
              "      <td>очевидцем</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68942</th>\n",
              "      <td>Тибу</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63651</th>\n",
              "      <td>снабжении</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61391</th>\n",
              "      <td>Сдвигу</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50249</th>\n",
              "      <td>Полозья</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56149</th>\n",
              "      <td>Радомиром</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43878</th>\n",
              "      <td>отбор</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13808</th>\n",
              "      <td>ГОРДОН</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55891</th>\n",
              "      <td>пёрышки</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44966</th>\n",
              "      <td>Паварами</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54798</th>\n",
              "      <td>просёлкам</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67415</th>\n",
              "      <td>съемкам</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80407</th>\n",
              "      <td>ярлычков</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33773</th>\n",
              "      <td>Манифестация</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17404</th>\n",
              "      <td>диссиденту</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7945</th>\n",
              "      <td>варранты</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57195</th>\n",
              "      <td>Распродажи</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43323</th>\n",
              "      <td>ориентациях</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37184</th>\n",
              "      <td>мускулатуры</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6821</th>\n",
              "      <td>брендинге</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10052</th>\n",
              "      <td>ВОЗМОЖНОСТЯХ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70568</th>\n",
              "      <td>Турбиных</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9957</th>\n",
              "      <td>Возглас</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63439</th>\n",
              "      <td>Слушаеве</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37896</th>\n",
              "      <td>наименованиями</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4151</th>\n",
              "      <td>Бараташвили</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11431</th>\n",
              "      <td>выпрямители</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26431</th>\n",
              "      <td>КЕНОМ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22976</th>\n",
              "      <td>Ииуй</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30569</th>\n",
              "      <td>КУВАЛДА</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17991</th>\n",
              "      <td>Доминионах</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50966</th>\n",
              "      <td>порфир</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41859</th>\n",
              "      <td>Ованесом</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61552</th>\n",
              "      <td>секансу</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32546</th>\n",
              "      <td>ЛОКАЛИЗАЦИЯ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24284</th>\n",
              "      <td>искуплении</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18612</th>\n",
              "      <td>ДУМ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55316</th>\n",
              "      <td>псарнях</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78276</th>\n",
              "      <td>щепотью</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70940</th>\n",
              "      <td>уборку</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15281</th>\n",
              "      <td>ДВИГАТЕЛЯМИ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57138</th>\n",
              "      <td>распознание</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65702</th>\n",
              "      <td>Старательность</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40107</th>\n",
              "      <td>нечитабельность</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37742</th>\n",
              "      <td>надмножество</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  word  label\n",
              "9035      видеокассета      0\n",
              "9032       видеокартах      0\n",
              "66634           стужей      0\n",
              "32053            ЛИДИЯ      0\n",
              "24413     исправлением      0\n",
              "44867        очевидцем      0\n",
              "68942             Тибу      0\n",
              "63651        снабжении      0\n",
              "61391           Сдвигу      0\n",
              "50249          Полозья      0\n",
              "56149        Радомиром      0\n",
              "43878            отбор      0\n",
              "13808           ГОРДОН      0\n",
              "55891          пёрышки      0\n",
              "44966         Паварами      1\n",
              "54798        просёлкам      0\n",
              "67415          съемкам      0\n",
              "80407         ярлычков      0\n",
              "33773     Манифестация      0\n",
              "17404       диссиденту      0\n",
              "7945          варранты      0\n",
              "57195       Распродажи      0\n",
              "43323      ориентациях      0\n",
              "37184      мускулатуры      0\n",
              "6821         брендинге      0\n",
              "10052     ВОЗМОЖНОСТЯХ      0\n",
              "70568         Турбиных      1\n",
              "9957           Возглас      0\n",
              "63439         Слушаеве      1\n",
              "37896   наименованиями      0\n",
              "4151       Бараташвили      1\n",
              "11431      выпрямители      0\n",
              "26431            КЕНОМ      0\n",
              "22976             Ииуй      0\n",
              "30569          КУВАЛДА      1\n",
              "17991       Доминионах      0\n",
              "50966           порфир      0\n",
              "41859         Ованесом      0\n",
              "61552          секансу      0\n",
              "32546      ЛОКАЛИЗАЦИЯ      0\n",
              "24284       искуплении      0\n",
              "18612              ДУМ      0\n",
              "55316          псарнях      0\n",
              "78276          щепотью      0\n",
              "70940           уборку      0\n",
              "15281      ДВИГАТЕЛЯМИ      0\n",
              "57138      распознание      0\n",
              "65702   Старательность      0\n",
              "40107  нечитабельность      0\n",
              "37742     надмножество      0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-q1JHzW1sd-"
      },
      "source": [
        "Теперь нам нужно собрать все символы в словарь. Лучше сразу посчитать количество упоминаний, чтобы отсеять самые редкие."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8t4nDCR1sd-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9acca2c-31d3-463d-a2d2-6bcdc127db49"
      },
      "source": [
        "vocab = Counter()\n",
        "for symbol in data['word']:\n",
        "    vocab.update(list(symbol))\n",
        "print('всего уникальных символов:', len(vocab))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFlVinuMObSL",
        "outputId": "fed8ab1c-c343-4b90-f432-6a27190e3cba"
      },
      "source": [
        "vocab"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'А': 7493,\n",
              "         'а': 57886,\n",
              "         'р': 39631,\n",
              "         'о': 59076,\n",
              "         'н': 41649,\n",
              "         'Р': 5636,\n",
              "         'О': 7225,\n",
              "         'Н': 5462,\n",
              "         'е': 54153,\n",
              "         'в': 23485,\n",
              "         'х': 6022,\n",
              "         'б': 8440,\n",
              "         'к': 26522,\n",
              "         'ж': 4331,\n",
              "         'у': 17169,\n",
              "         'м': 26553,\n",
              "         'ы': 7465,\n",
              "         'й': 7297,\n",
              "         'д': 15510,\n",
              "         'л': 25494,\n",
              "         'и': 58252,\n",
              "         'т': 36412,\n",
              "         'Б': 2967,\n",
              "         'Ш': 858,\n",
              "         'ш': 3678,\n",
              "         'з': 9072,\n",
              "         'с': 28097,\n",
              "         'ю': 4854,\n",
              "         'г': 8524,\n",
              "         'ч': 5866,\n",
              "         'Д': 3385,\n",
              "         'Л': 3676,\n",
              "         'п': 15241,\n",
              "         'Х': 1214,\n",
              "         'Г': 2395,\n",
              "         'Е': 5981,\n",
              "         'У': 2215,\n",
              "         'В': 4200,\n",
              "         'Ы': 787,\n",
              "         'М': 4175,\n",
              "         'Ж': 720,\n",
              "         'И': 7304,\n",
              "         'Ч': 1013,\n",
              "         'ц': 6590,\n",
              "         'я': 9339,\n",
              "         'ь': 7634,\n",
              "         'К': 5403,\n",
              "         'Й': 763,\n",
              "         'Ю': 550,\n",
              "         'З': 1775,\n",
              "         'С': 5917,\n",
              "         'Т': 5027,\n",
              "         '-': 801,\n",
              "         'Ь': 706,\n",
              "         'щ': 1981,\n",
              "         'Я': 1401,\n",
              "         ' ': 106,\n",
              "         'Ф': 1100,\n",
              "         'э': 1683,\n",
              "         'П': 5068,\n",
              "         'ф': 3821,\n",
              "         'Ц': 809,\n",
              "         'ъ': 216,\n",
              "         'ё': 727,\n",
              "         'Э': 863,\n",
              "         'Ё': 27,\n",
              "         'Щ': 237,\n",
              "         '.': 53,\n",
              "         '·': 2,\n",
              "         'ó': 3,\n",
              "         '/': 6,\n",
              "         '“': 2,\n",
              "         '”': 2,\n",
              "         '7': 1,\n",
              "         'Ъ': 28,\n",
              "         \"'\": 4,\n",
              "         '’': 4,\n",
              "         '«': 4,\n",
              "         '»': 2,\n",
              "         '2': 2,\n",
              "         'ѐ': 1,\n",
              "         'é': 1,\n",
              "         '3': 4,\n",
              "         '4': 1,\n",
              "         'Ó': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wKHM_LU1seA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c2891d-2daa-4c43-8a7d-7b0e4f10d807"
      },
      "source": [
        "filtered_vocab = set()\n",
        "\n",
        "for symbol in vocab:\n",
        "    if vocab[symbol] > 5:\n",
        "        filtered_vocab.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_vocab))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных символов, втретившихся больше 5 раз: 70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEUhJv5N1seC"
      },
      "source": [
        "#создаем словарь с индексами symbol2id, для спецсимвола паддинга дефолтный индекс - 0\n",
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_vocab:\n",
        "    symbol2id[symbol] = len(symbol2id)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6J89I9d1seC"
      },
      "source": [
        "#обратный словарь для того, чтобы раскодировать последовательность\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrzM7MnCQeP_",
        "outputId": "5ecd631f-a97a-438c-b9d2-3f994eb9a806"
      },
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obiXRWLt1OZJ"
      },
      "source": [
        "### Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMs4ZohJ1seI"
      },
      "source": [
        "class SurnamesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, symbol2id, DEVICE):\n",
        "        self.dataset = dataset['word'].values\n",
        "        self.symbol2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['label'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        symbols = list(self.dataset[index])\n",
        "        ids = torch.LongTensor([self.symbol2id[symbol] for symbol in symbols if symbol in self.symbol2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_ids, y"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Wjfyxar7U1"
      },
      "source": [
        "### создаем итераторы по данным для трейна и теста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zl4FxB71seI"
      },
      "source": [
        "train_dataset = SurnamesDataset(train_data, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xauLXZwQNcCq"
      },
      "source": [
        "batch = next(iter(train_iterator))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOqAu8X8S36K",
        "outputId": "b0d6b0b8-b334-4bdf-8181-f0b4324d747c"
      },
      "source": [
        "batch[0].shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1024, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZbD27_A6Nck",
        "outputId": "14d5a1f5-6037-4976-cff2-60f24266ff5d"
      },
      "source": [
        "[id2symbol[int(i)] for i in batch[0][0]]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Н',\n",
              " 'е',\n",
              " 'д',\n",
              " 'е',\n",
              " 'р',\n",
              " 'ж',\n",
              " 'а',\n",
              " 'н',\n",
              " 'и',\n",
              " 'е',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj6Wltq3TfGQ",
        "outputId": "7eb93dea-f5f6-4748-b9db-70bf93b82de3"
      },
      "source": [
        "batch[1]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        ...,\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR_upb-_1seJ"
      },
      "source": [
        "val_dataset = SurnamesDataset(val_data, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQlpcUR28V-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c82a34-0bd9-4968-a2f6-ce2c261e8a19"
      },
      "source": [
        "test_batch = next(iter(val_iterator))\n",
        "test_batch[0].shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1024, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUCT8ayD1seK"
      },
      "source": [
        "### CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfVEoM-IZFFe",
        "outputId": "8f421f2e-f59e-4eb1-fc81-1d45ff47da1e"
      },
      "source": [
        "fm = torch.randn(3, 8, 4) #batch_size, num_filters, seq_len\n",
        "fm"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1885, -0.1980,  1.3126, -0.1025],\n",
              "         [ 1.5409,  0.1547, -0.7873, -0.1500],\n",
              "         [ 0.4081,  0.2194, -0.1682, -0.3834],\n",
              "         [-0.2095,  0.6825,  2.0395, -0.1677],\n",
              "         [ 0.3436, -0.9711,  1.0768,  0.5207],\n",
              "         [ 0.3810,  1.4883,  0.3429, -0.0839],\n",
              "         [ 0.7323, -3.1354, -1.3049,  0.8909],\n",
              "         [-0.3939,  0.1204, -0.2567,  0.3816]],\n",
              "\n",
              "        [[-0.1963, -1.9160, -0.4341,  0.1044],\n",
              "         [ 0.0375, -0.2647, -0.2494, -0.1921],\n",
              "         [ 2.4131, -0.1584, -0.5153, -1.0434],\n",
              "         [ 1.6581,  0.7602,  0.2053,  0.8443],\n",
              "         [-0.1549, -0.3073,  1.1251, -1.5624],\n",
              "         [ 0.0067, -0.2508,  0.2348, -0.8780],\n",
              "         [ 1.8899,  0.2995, -1.1775,  0.5583],\n",
              "         [-1.4322,  0.8759,  1.9928,  1.8875]],\n",
              "\n",
              "        [[ 1.4176, -0.8858,  0.2661, -0.4785],\n",
              "         [-0.2453,  0.2560,  1.6534, -1.2000],\n",
              "         [-0.5488, -1.5365, -0.1259,  0.0101],\n",
              "         [-1.9550,  0.6945, -0.9071, -1.5170],\n",
              "         [ 1.5609,  0.8492, -1.2659,  0.2130],\n",
              "         [-1.4127,  1.7286,  2.0900, -0.8393],\n",
              "         [-0.3401,  1.0670, -0.3403,  0.5879],\n",
              "         [ 1.7755, -0.0565, -1.3129, -0.8158]]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXXF4sU2ZFy4",
        "outputId": "84b83853-bd68-424a-db1b-71bb2f2a6862"
      },
      "source": [
        "mp = torch.nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "print(mp(fm).shape)\n",
        "mp(fm)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 8, 2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1885,  1.3126],\n",
              "         [ 1.5409, -0.1500],\n",
              "         [ 0.4081, -0.1682],\n",
              "         [ 0.6825,  2.0395],\n",
              "         [ 0.3436,  1.0768],\n",
              "         [ 1.4883,  0.3429],\n",
              "         [ 0.7323,  0.8909],\n",
              "         [ 0.1204,  0.3816]],\n",
              "\n",
              "        [[-0.1963,  0.1044],\n",
              "         [ 0.0375, -0.1921],\n",
              "         [ 2.4131, -0.5153],\n",
              "         [ 1.6581,  0.8443],\n",
              "         [-0.1549,  1.1251],\n",
              "         [ 0.0067,  0.2348],\n",
              "         [ 1.8899,  0.5583],\n",
              "         [ 0.8759,  1.9928]],\n",
              "\n",
              "        [[ 1.4176,  0.2661],\n",
              "         [ 0.2560,  1.6534],\n",
              "         [-0.5488,  0.0101],\n",
              "         [ 0.6945, -0.9071],\n",
              "         [ 1.5609,  0.2130],\n",
              "         [ 1.7286,  2.0900],\n",
              "         [ 1.0670,  0.5879],\n",
              "         [ 1.7755, -0.8158]]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JZy7ZDlctwM",
        "outputId": "b1696999-a3c0-4262-cb53-41bf8905754d"
      },
      "source": [
        "fm.max(2)[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.3126, 1.5409, 0.4081, 2.0395, 1.0768, 1.4883, 0.8909, 0.3816],\n",
              "        [0.1044, 0.0375, 2.4131, 1.6581, 1.1251, 0.2348, 1.8899, 1.9928],\n",
              "        [1.4176, 1.6534, 0.0101, 0.6945, 1.5609, 2.0900, 1.0670, 1.7755]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdV7oSIc1seK"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        #batch_size x seq_len\n",
        "        embedded = self.embedding(word)\n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.dropout(self.pooling(self.relu(self.bigrams(embedded))))\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams(embedded))))\n",
        "        #batch_size x filter_count3 x seq_len*\n",
        "\n",
        "        pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        # batch_size x filter_count2\n",
        "        pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        # batch_size x filter_count3\n",
        "        concat = torch.cat((pooling1, pooling2), 1)\n",
        "        # batch _size x (filter_count2 + filter_count3)\n",
        "        logits = self.hidden(concat) \n",
        "        logits = self.out(logits)      \n",
        "        return logits\n",
        "      \n",
        "        \n",
        "    "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFQ-WEY4cxkv",
        "outputId": "d8a84c76-aad6-4529-a0bf-0eba0fe18e02"
      },
      "source": [
        "batch, y = next(iter(train_iterator))\n",
        "batch, y = batch.to(device='cpu'), y.to(device='cpu')\n",
        "print(batch)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2, 50, 15,  ...,  0,  0,  0],\n",
            "        [ 1, 47, 24,  ...,  0,  0,  0],\n",
            "        [57,  1, 62,  ...,  0,  0,  0],\n",
            "        ...,\n",
            "        [60, 65, 17,  ...,  0,  0,  0],\n",
            "        [53, 32, 35,  ...,  0,  0,  0],\n",
            "        [18, 58, 46,  ...,  0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSmsX_fg4LcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd4eec0-cf9c-4ad0-a016-d9301b1cd055"
      },
      "source": [
        "y"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        ...,\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHc9E_96dJBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906c52ad-5e04-4586-990f-b5e9adc6edbd"
      },
      "source": [
        "\n",
        "model = CNN(len(id2symbol), 8)\n",
        "output = model(batch)\n",
        "print(output)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3674],\n",
            "        [0.4344],\n",
            "        [0.3513],\n",
            "        ...,\n",
            "        [0.5523],\n",
            "        [0.6195],\n",
            "        [0.5249]], grad_fn=<SigmoidBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py:298: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:647.)\n",
            "  self.padding, self.dilation, self.groups)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBWdfROdQK5v",
        "outputId": "93408385-6f5c-45f7-e6ed-d2feafd8d6c7"
      },
      "source": [
        "loss = nn.BCELoss()\n",
        "loss(output, y)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7087, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q73ELWxPzjtZ",
        "outputId": "5a18feb0-b932-407f-8a2e-02b1fa41e8b5"
      },
      "source": [
        "f1(output, y.long())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.2058)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYz0OzYT1vt1"
      },
      "source": [
        "### training loop, логика обучения и валидации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8sZOK7Kvk7o"
      },
      "source": [
        "теперь нам нужны функции для обучения и валидации,\n",
        "каждый вызов функции - одна эпоха обучения \n",
        "\n",
        "За одну эпоху нам надо для каждого батча:\n",
        "\n",
        "-- применить к нему модель, \n",
        "\n",
        "-- посчитать значение функции потерь, \n",
        "\n",
        "-- посчитать градиенты,\n",
        "\n",
        "-- обновить веса (параметры модели)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVKQzPPI1seJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(texts)  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPfO7p9x1seK"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRoSupMNQsvF"
      },
      "source": [
        "### инициализируем модель, задаем оптимизатор и функцию потерь"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO35IZES1seL"
      },
      "source": [
        "model = CNN(len(symbol2id), 8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kHbVX0y1seL"
      },
      "source": [
        "### запуск обучения!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boMAjlVM1seM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2925b1a-08cc-447a-fa61-0199f7575432"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(50):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.5100443715398962\n",
            "Train loss: 0.44168137726576434\n",
            "Train loss: 0.41651446393557956\n",
            "Train loss: 0.4018380844846685\n",
            "Train loss: 0.39637994968284995\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.43682366609573364, Val f1: 0.0\n",
            "Val loss: 0.41659214056056476, Val f1: 0.0\n",
            "Val loss: 0.4100355701787131, Val f1: 0.0\n",
            "Val loss: 0.4082566043163868, Val f1: 0.0\n",
            "Val loss: 0.40683456952289, Val f1: 0.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6024087369441986, Val f1: 0.0\n",
            "Val loss: 0.484754478931427, Val f1: 0.0\n",
            "Val loss: 0.45419665426015854, Val f1: 0.0\n",
            "Val loss: 0.4424824795939706, Val f1: 0.0\n",
            "Val loss: 0.43295191867010935, Val f1: 0.0\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.39992029016668146\n",
            "Train loss: 0.37380144259204034\n",
            "Train loss: 0.36673656531742643\n",
            "Train loss: 0.3615335891855524\n",
            "Train loss: 0.3593475924710096\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.41747277162291785, Val f1: 0.0\n",
            "Val loss: 0.3968826493491297, Val f1: 0.0\n",
            "Val loss: 0.39435156754084993, Val f1: 0.0\n",
            "Val loss: 0.39118095091048705, Val f1: 0.0\n",
            "Val loss: 0.3888788667775817, Val f1: 0.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5746559202671051, Val f1: 0.0\n",
            "Val loss: 0.4631007254123688, Val f1: 0.0\n",
            "Val loss: 0.4338376745581627, Val f1: 0.0\n",
            "Val loss: 0.4223813333294608, Val f1: 0.0\n",
            "Val loss: 0.41316430270671844, Val f1: 0.0\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.3715031255375255\n",
            "Train loss: 0.3540313062460526\n",
            "Train loss: 0.3511423707008362\n",
            "Train loss: 0.3488204238262582\n",
            "Train loss: 0.3469178600836608\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4105784757570787, Val f1: 0.0\n",
            "Val loss: 0.3869775417058364, Val f1: 0.0\n",
            "Val loss: 0.3817713115896497, Val f1: 0.0\n",
            "Val loss: 0.3778686453687384, Val f1: 0.0\n",
            "Val loss: 0.37502282251745966, Val f1: 0.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5539688915014267, Val f1: 0.0\n",
            "Val loss: 0.4471999228000641, Val f1: 0.0\n",
            "Val loss: 0.418735858052969, Val f1: 0.0\n",
            "Val loss: 0.40739417347041046, Val f1: 0.0012714557815343142\n",
            "Val loss: 0.3984802003417696, Val f1: 0.0009990009712055326\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.35996266386725684\n",
            "Train loss: 0.35049420983895013\n",
            "Train loss: 0.3451577237674168\n",
            "Train loss: 0.34010875605522317\n",
            "Train loss: 0.33813042307304125\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3910698944872076, Val f1: 0.0\n",
            "Val loss: 0.37347887262054114, Val f1: 0.0028641088865697384\n",
            "Val loss: 0.36576141800199236, Val f1: 0.0028796133119612932\n",
            "Val loss: 0.3641200541181767, Val f1: 0.0039728619158267975\n",
            "Val loss: 0.36238184722803407, Val f1: 0.003998670261353254\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.535004585981369, Val f1: 0.0\n",
            "Val loss: 0.43271756172180176, Val f1: 0.0\n",
            "Val loss: 0.4049723520874977, Val f1: 0.0\n",
            "Val loss: 0.39379592646252026, Val f1: 0.0012626262614503503\n",
            "Val loss: 0.3851468563079834, Val f1: 0.0009920635493472219\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.35145059498873626\n",
            "Train loss: 0.34192556920258893\n",
            "Train loss: 0.33705867954662866\n",
            "Train loss: 0.33272333855324604\n",
            "Train loss: 0.33210228610846954\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3804331692782315, Val f1: 0.02134126052260399\n",
            "Val loss: 0.36172763549763226, Val f1: 0.023470843210816383\n",
            "Val loss: 0.35909394196101596, Val f1: 0.021122891455888748\n",
            "Val loss: 0.35484401279307426, Val f1: 0.01889006234705448\n",
            "Val loss: 0.3536670314053358, Val f1: 0.019081495702266693\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5224521458148956, Val f1: 0.0\n",
            "Val loss: 0.4232261836528778, Val f1: 0.009184150025248528\n",
            "Val loss: 0.39582838118076324, Val f1: 0.013353572227060795\n",
            "Val loss: 0.38468772714788263, Val f1: 0.013447679579257965\n",
            "Val loss: 0.37628198308604105, Val f1: 0.011727473698556423\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.3452421751889316\n",
            "Train loss: 0.3304416938968327\n",
            "Train loss: 0.32977116448538646\n",
            "Train loss: 0.3259097090426912\n",
            "Train loss: 0.32406766131772835\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3733498616652055, Val f1: 0.07480435818433762\n",
            "Val loss: 0.35768304570861487, Val f1: 0.0647643506526947\n",
            "Val loss: 0.3536665192672184, Val f1: 0.06136009469628334\n",
            "Val loss: 0.34929385971515736, Val f1: 0.05845928564667702\n",
            "Val loss: 0.3471823041721926, Val f1: 0.06046327203512192\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5118793249130249, Val f1: 0.1260022521018982\n",
            "Val loss: 0.4153308570384979, Val f1: 0.08288553357124329\n",
            "Val loss: 0.38819508999586105, Val f1: 0.07627277821302414\n",
            "Val loss: 0.3770479234782132, Val f1: 0.0769069492816925\n",
            "Val loss: 0.3688756802252361, Val f1: 0.06798546761274338\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.343198532407934\n",
            "Train loss: 0.3267112555711166\n",
            "Train loss: 0.3202763157231467\n",
            "Train loss: 0.3176154031398449\n",
            "Train loss: 0.31772675908217995\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3707024942744862, Val f1: 0.11021889746189117\n",
            "Val loss: 0.3497416921283888, Val f1: 0.11365577578544617\n",
            "Val loss: 0.34496106590543474, Val f1: 0.1019420474767685\n",
            "Val loss: 0.3415281151203399, Val f1: 0.10000330209732056\n",
            "Val loss: 0.34038039486287003, Val f1: 0.10460732132196426\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5021621137857437, Val f1: 0.17497603595256805\n",
            "Val loss: 0.40808846354484557, Val f1: 0.12718652188777924\n",
            "Val loss: 0.38108452036976814, Val f1: 0.11779859662055969\n",
            "Val loss: 0.36996727098118176, Val f1: 0.11554466933012009\n",
            "Val loss: 0.36201844045094084, Val f1: 0.11284109950065613\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.3304718461903659\n",
            "Train loss: 0.31982276621072186\n",
            "Train loss: 0.31539237073489595\n",
            "Train loss: 0.31376504390797716\n",
            "Train loss: 0.31322744539228536\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.35265059633688495, Val f1: 0.1686701774597168\n",
            "Val loss: 0.3358831496342369, Val f1: 0.14653289318084717\n",
            "Val loss: 0.33154942308153423, Val f1: 0.14704039692878723\n",
            "Val loss: 0.3310432751128014, Val f1: 0.14264434576034546\n",
            "Val loss: 0.33133198851245943, Val f1: 0.14091750979423523\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.48735982179641724, Val f1: 0.21434932947158813\n",
            "Val loss: 0.39680551886558535, Val f1: 0.18208591639995575\n",
            "Val loss: 0.3702545538544655, Val f1: 0.1665448546409607\n",
            "Val loss: 0.35935737599026074, Val f1: 0.16135387122631073\n",
            "Val loss: 0.3516127658741815, Val f1: 0.1559302657842636\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.32773753187873145\n",
            "Train loss: 0.3165971882965254\n",
            "Train loss: 0.31223596675055365\n",
            "Train loss: 0.30888841887737845\n",
            "Train loss: 0.3068901674222138\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3512286733497273, Val f1: 0.18851803243160248\n",
            "Val loss: 0.3363389061844867, Val f1: 0.17885388433933258\n",
            "Val loss: 0.33012154357773915, Val f1: 0.1792297065258026\n",
            "Val loss: 0.3276672471076884, Val f1: 0.17920057475566864\n",
            "Val loss: 0.3245760313535141, Val f1: 0.1792820245027542\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.47768762707710266, Val f1: 0.2889334559440613\n",
            "Val loss: 0.3895056188106537, Val f1: 0.21716511249542236\n",
            "Val loss: 0.36318084597587585, Val f1: 0.20021004974842072\n",
            "Val loss: 0.3524074175141074, Val f1: 0.19502609968185425\n",
            "Val loss: 0.3448396303824016, Val f1: 0.18850591778755188\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.3310918076471849\n",
            "Train loss: 0.3158253366532533\n",
            "Train loss: 0.3075011389596122\n",
            "Train loss: 0.3052920749846925\n",
            "Train loss: 0.30282097549761755\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.34422827850688587, Val f1: 0.26369768381118774\n",
            "Val loss: 0.335010756617007, Val f1: 0.2537824809551239\n",
            "Val loss: 0.3278118704046522, Val f1: 0.24617315828800201\n",
            "Val loss: 0.32448318854291386, Val f1: 0.2464263141155243\n",
            "Val loss: 0.32311312323909697, Val f1: 0.24343635141849518\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4760599583387375, Val f1: 0.3587794601917267\n",
            "Val loss: 0.3887190163135529, Val f1: 0.2775142788887024\n",
            "Val loss: 0.3622179217636585, Val f1: 0.2603042721748352\n",
            "Val loss: 0.35128064318136737, Val f1: 0.25777339935302734\n",
            "Val loss: 0.3437986373901367, Val f1: 0.24811558425426483\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.3139395605434071\n",
            "Train loss: 0.3037721177806025\n",
            "Train loss: 0.298805045230048\n",
            "Train loss: 0.2984519340890519\n",
            "Train loss: 0.29795883772736886\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3419100588018244, Val f1: 0.29469195008277893\n",
            "Val loss: 0.325058005426241, Val f1: 0.2608477473258972\n",
            "Val loss: 0.3197689124516078, Val f1: 0.2503034472465515\n",
            "Val loss: 0.31607973829228825, Val f1: 0.2514578700065613\n",
            "Val loss: 0.3157810068736642, Val f1: 0.2508813142776489\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4635746479034424, Val f1: 0.38724029064178467\n",
            "Val loss: 0.3791794776916504, Val f1: 0.29057776927948\n",
            "Val loss: 0.3531853146851063, Val f1: 0.27535706758499146\n",
            "Val loss: 0.3424759534272281, Val f1: 0.2690815329551697\n",
            "Val loss: 0.33515884620802744, Val f1: 0.2618171274662018\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.32016660408540204\n",
            "Train loss: 0.30062333526818646\n",
            "Train loss: 0.29777909091540744\n",
            "Train loss: 0.2957891618951838\n",
            "Train loss: 0.29482727131601105\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.33561043305830524, Val f1: 0.29799818992614746\n",
            "Val loss: 0.31841446005779767, Val f1: 0.29296255111694336\n",
            "Val loss: 0.3146572070462363, Val f1: 0.2904416024684906\n",
            "Val loss: 0.31248364423183683, Val f1: 0.2885484993457794\n",
            "Val loss: 0.3115525983147702, Val f1: 0.28617337346076965\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.45739854872226715, Val f1: 0.42215076088905334\n",
            "Val loss: 0.3747082531452179, Val f1: 0.32220447063446045\n",
            "Val loss: 0.3489282689988613, Val f1: 0.30090630054473877\n",
            "Val loss: 0.3381845842708241, Val f1: 0.2974070906639099\n",
            "Val loss: 0.3309153424842017, Val f1: 0.2934468686580658\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.31818694418126886\n",
            "Train loss: 0.3031502508598825\n",
            "Train loss: 0.29685470887592863\n",
            "Train loss: 0.29403394967951674\n",
            "Train loss: 0.2906791870876894\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3239100656726144, Val f1: 0.3384261429309845\n",
            "Val loss: 0.3114566738190858, Val f1: 0.31964927911758423\n",
            "Val loss: 0.30867459944316317, Val f1: 0.31408214569091797\n",
            "Val loss: 0.30776093297816337, Val f1: 0.3120729923248291\n",
            "Val loss: 0.306822549993709, Val f1: 0.31078284978866577\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.45008762180805206, Val f1: 0.44763854146003723\n",
            "Val loss: 0.3693893551826477, Val f1: 0.335485577583313\n",
            "Val loss: 0.3438916951417923, Val f1: 0.3165091574192047\n",
            "Val loss: 0.3332758220759305, Val f1: 0.3182472884654999\n",
            "Val loss: 0.3261408231088093, Val f1: 0.31903958320617676\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.3118054270744324\n",
            "Train loss: 0.2938397656316343\n",
            "Train loss: 0.28891707956790924\n",
            "Train loss: 0.28697786717972856\n",
            "Train loss: 0.2872651055202646\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3290190507065166, Val f1: 0.36191657185554504\n",
            "Val loss: 0.31164481328881305, Val f1: 0.3491131067276001\n",
            "Val loss: 0.3077537587710789, Val f1: 0.3486844599246979\n",
            "Val loss: 0.3059936977447347, Val f1: 0.3435203433036804\n",
            "Val loss: 0.30475366923768643, Val f1: 0.3504616320133209\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.44693510234355927, Val f1: 0.5232325196266174\n",
            "Val loss: 0.3672774851322174, Val f1: 0.39312130212783813\n",
            "Val loss: 0.341867845505476, Val f1: 0.37252095341682434\n",
            "Val loss: 0.33120193806561554, Val f1: 0.3763757348060608\n",
            "Val loss: 0.3240926797900881, Val f1: 0.3727910816669464\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.29902220043269073\n",
            "Train loss: 0.28371507123760553\n",
            "Train loss: 0.2827944070100784\n",
            "Train loss: 0.28342339364772146\n",
            "Train loss: 0.28326410737078067\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3193506354635412, Val f1: 0.40317368507385254\n",
            "Val loss: 0.3075421711672907, Val f1: 0.3782172203063965\n",
            "Val loss: 0.30321404082434517, Val f1: 0.3872898817062378\n",
            "Val loss: 0.3017861589472345, Val f1: 0.3838834762573242\n",
            "Val loss: 0.30221412292981553, Val f1: 0.3829081654548645\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.44350363314151764, Val f1: 0.5664570331573486\n",
            "Val loss: 0.3648615777492523, Val f1: 0.4238525331020355\n",
            "Val loss: 0.33953993022441864, Val f1: 0.4141932725906372\n",
            "Val loss: 0.328845739364624, Val f1: 0.4123828411102295\n",
            "Val loss: 0.3218551639999662, Val f1: 0.40688270330429077\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.3042260327122428\n",
            "Train loss: 0.28597409958424774\n",
            "Train loss: 0.28236501131738934\n",
            "Train loss: 0.2808873044683578\n",
            "Train loss: 0.28050259109270775\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3190087919885462, Val f1: 0.4180479943752289\n",
            "Val loss: 0.3046794663304868, Val f1: 0.4028106927871704\n",
            "Val loss: 0.29878163763454985, Val f1: 0.4021637737751007\n",
            "Val loss: 0.29814256315535687, Val f1: 0.4010365605354309\n",
            "Val loss: 0.29801210407483375, Val f1: 0.39560195803642273\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4374464750289917, Val f1: 0.5962370038032532\n",
            "Val loss: 0.3602059781551361, Val f1: 0.44906434416770935\n",
            "Val loss: 0.3351886197924614, Val f1: 0.4323347210884094\n",
            "Val loss: 0.32457830147309735, Val f1: 0.42903804779052734\n",
            "Val loss: 0.3176529662949698, Val f1: 0.4225870668888092\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.2840517650951039\n",
            "Train loss: 0.28136439427085547\n",
            "Train loss: 0.28054788112640383\n",
            "Train loss: 0.27940103538492894\n",
            "Train loss: 0.27931948435508597\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3147136704488234, Val f1: 0.4667697846889496\n",
            "Val loss: 0.3040362518766652, Val f1: 0.43170225620269775\n",
            "Val loss: 0.3019616561276572, Val f1: 0.41907191276550293\n",
            "Val loss: 0.2986244426128712, Val f1: 0.4236455261707306\n",
            "Val loss: 0.29751058200658376, Val f1: 0.4206761121749878\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4355718046426773, Val f1: 0.6468664407730103\n",
            "Val loss: 0.3590601205825806, Val f1: 0.4836832582950592\n",
            "Val loss: 0.33407290652394295, Val f1: 0.46655088663101196\n",
            "Val loss: 0.32343928109515796, Val f1: 0.45768022537231445\n",
            "Val loss: 0.3165108008044107, Val f1: 0.4524177610874176\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.29286613247611304\n",
            "Train loss: 0.282085499037867\n",
            "Train loss: 0.2774074550185885\n",
            "Train loss: 0.2767524652658625\n",
            "Train loss: 0.27556469607151163\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.31625957651571796, Val f1: 0.48415428400039673\n",
            "Val loss: 0.3076695592507072, Val f1: 0.47068697214126587\n",
            "Val loss: 0.30408717223576137, Val f1: 0.4695397615432739\n",
            "Val loss: 0.30354835758817955, Val f1: 0.4597623944282532\n",
            "Val loss: 0.30231060971648005, Val f1: 0.45895370841026306\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.44170692563056946, Val f1: 0.7058913707733154\n",
            "Val loss: 0.36414566040039065, Val f1: 0.5343817472457886\n",
            "Val loss: 0.33889689296483994, Val f1: 0.507538914680481\n",
            "Val loss: 0.3279572779482061, Val f1: 0.49811387062072754\n",
            "Val loss: 0.3210557848215103, Val f1: 0.49546653032302856\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.2932301976464011\n",
            "Train loss: 0.27908676603566046\n",
            "Train loss: 0.2752383083105087\n",
            "Train loss: 0.2744699288555916\n",
            "Train loss: 0.27140745899434815\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.31355429779399524, Val f1: 0.4966765344142914\n",
            "Val loss: 0.2990543570207513, Val f1: 0.4700419008731842\n",
            "Val loss: 0.29525273953165326, Val f1: 0.46623367071151733\n",
            "Val loss: 0.29522269647172156, Val f1: 0.4640340507030487\n",
            "Val loss: 0.2940899563037743, Val f1: 0.46348148584365845\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4312591254711151, Val f1: 0.7115080952644348\n",
            "Val loss: 0.3560844361782074, Val f1: 0.5343005657196045\n",
            "Val loss: 0.3311782069504261, Val f1: 0.5078765153884888\n",
            "Val loss: 0.32041992382569745, Val f1: 0.4984225928783417\n",
            "Val loss: 0.3135328527007784, Val f1: 0.49604475498199463\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.2869137159802697\n",
            "Train loss: 0.277805063387622\n",
            "Train loss: 0.27471352304731095\n",
            "Train loss: 0.2722306819038188\n",
            "Train loss: 0.2711573685124769\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3137783814560283, Val f1: 0.5165794491767883\n",
            "Val loss: 0.30051603783731873, Val f1: 0.4927089214324951\n",
            "Val loss: 0.2957747135843549, Val f1: 0.49297574162483215\n",
            "Val loss: 0.2944481930834182, Val f1: 0.48184311389923096\n",
            "Val loss: 0.29338789541842575, Val f1: 0.4760916829109192\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4297334551811218, Val f1: 0.7429689764976501\n",
            "Val loss: 0.3550498068332672, Val f1: 0.5595975518226624\n",
            "Val loss: 0.33023060113191605, Val f1: 0.5278805494308472\n",
            "Val loss: 0.31952585957267066, Val f1: 0.5151940584182739\n",
            "Val loss: 0.3127106555870601, Val f1: 0.5099809765815735\n",
            "\n",
            "starting Epoch 20\n",
            "Training...\n",
            "Train loss: 0.2855981087142771\n",
            "Train loss: 0.2811841517686844\n",
            "Train loss: 0.27616610527038576\n",
            "Train loss: 0.27301168568590856\n",
            "Train loss: 0.27061168672674796\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3000295514410192, Val f1: 0.5097032785415649\n",
            "Val loss: 0.2921849929768106, Val f1: 0.47903332114219666\n",
            "Val loss: 0.2890239877360208, Val f1: 0.4779112637042999\n",
            "Val loss: 0.2892111216453796, Val f1: 0.46795791387557983\n",
            "Val loss: 0.2866080509404005, Val f1: 0.4693971872329712\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4190684109926224, Val f1: 0.7152601480484009\n",
            "Val loss: 0.3466881036758423, Val f1: 0.5453559756278992\n",
            "Val loss: 0.32240793481469154, Val f1: 0.518203854560852\n",
            "Val loss: 0.311962674964558, Val f1: 0.5065997242927551\n",
            "Val loss: 0.30513620589460644, Val f1: 0.502107560634613\n",
            "\n",
            "starting Epoch 21\n",
            "Training...\n",
            "Train loss: 0.2957719591530887\n",
            "Train loss: 0.28119850482629694\n",
            "Train loss: 0.27392182307583945\n",
            "Train loss: 0.27036843845184816\n",
            "Train loss: 0.2683401870525489\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.31161969900131226, Val f1: 0.521780788898468\n",
            "Val loss: 0.2954808914143106, Val f1: 0.5037356019020081\n",
            "Val loss: 0.2912218153476715, Val f1: 0.4980534613132477\n",
            "Val loss: 0.28954120836359387, Val f1: 0.48687100410461426\n",
            "Val loss: 0.28726325499809396, Val f1: 0.4895893633365631\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4203704595565796, Val f1: 0.7447881698608398\n",
            "Val loss: 0.34781461358070376, Val f1: 0.5600249767303467\n",
            "Val loss: 0.32344067841768265, Val f1: 0.5315818786621094\n",
            "Val loss: 0.3129060891541568, Val f1: 0.5211694836616516\n",
            "Val loss: 0.3061323527778898, Val f1: 0.5167619585990906\n",
            "\n",
            "starting Epoch 22\n",
            "Training...\n",
            "Train loss: 0.28204107555476104\n",
            "Train loss: 0.26766720155011053\n",
            "Train loss: 0.26589972376823423\n",
            "Train loss: 0.26616316019220554\n",
            "Train loss: 0.26517203147128476\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3092194605957378, Val f1: 0.5376244783401489\n",
            "Val loss: 0.2926754083322442, Val f1: 0.5285717844963074\n",
            "Val loss: 0.28805519001824514, Val f1: 0.5224154591560364\n",
            "Val loss: 0.28786401292111014, Val f1: 0.5144925713539124\n",
            "Val loss: 0.28783667845241095, Val f1: 0.5091550350189209\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4214341789484024, Val f1: 0.7595852017402649\n",
            "Val loss: 0.3487232506275177, Val f1: 0.5712139010429382\n",
            "Val loss: 0.3243620879948139, Val f1: 0.5438538193702698\n",
            "Val loss: 0.3137931363149123, Val f1: 0.5323829650878906\n",
            "Val loss: 0.30700114582266125, Val f1: 0.5274494886398315\n",
            "\n",
            "starting Epoch 23\n",
            "Training...\n",
            "Train loss: 0.27406869557770813\n",
            "Train loss: 0.26711573808089545\n",
            "Train loss: 0.2638932372842516\n",
            "Train loss: 0.26321153691474425\n",
            "Train loss: 0.2630274967621949\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3058015189387582, Val f1: 0.5375275611877441\n",
            "Val loss: 0.2889503590438677, Val f1: 0.5126898288726807\n",
            "Val loss: 0.2840333546910967, Val f1: 0.5018897652626038\n",
            "Val loss: 0.2819923932882065, Val f1: 0.502088725566864\n",
            "Val loss: 0.2801560769646855, Val f1: 0.49948784708976746\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4108186364173889, Val f1: 0.7457981109619141\n",
            "Val loss: 0.3403245389461517, Val f1: 0.5657216310501099\n",
            "Val loss: 0.3164587654173374, Val f1: 0.5386806130409241\n",
            "Val loss: 0.30609762397679413, Val f1: 0.5256512761116028\n",
            "Val loss: 0.2992665618658066, Val f1: 0.522226095199585\n",
            "\n",
            "starting Epoch 24\n",
            "Training...\n",
            "Train loss: 0.26950428973544727\n",
            "Train loss: 0.2691211149744365\n",
            "Train loss: 0.26663199876035965\n",
            "Train loss: 0.2627675320873869\n",
            "Train loss: 0.26127812796730104\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2966034547849135, Val f1: 0.5569713711738586\n",
            "Val loss: 0.2880244384641233, Val f1: 0.5120143294334412\n",
            "Val loss: 0.28111203994069783, Val f1: 0.5107110142707825\n",
            "Val loss: 0.280485914108601, Val f1: 0.5062259435653687\n",
            "Val loss: 0.2784892949512449, Val f1: 0.5038158297538757\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.40772442519664764, Val f1: 0.7759746313095093\n",
            "Val loss: 0.3378899931907654, Val f1: 0.5773305296897888\n",
            "Val loss: 0.31421637535095215, Val f1: 0.5485914349555969\n",
            "Val loss: 0.3039543249390342, Val f1: 0.5327539443969727\n",
            "Val loss: 0.2971709072589874, Val f1: 0.5292847156524658\n",
            "\n",
            "starting Epoch 25\n",
            "Training...\n",
            "Train loss: 0.277542845769362\n",
            "Train loss: 0.26919353202633234\n",
            "Train loss: 0.2628762402704784\n",
            "Train loss: 0.26156699689144786\n",
            "Train loss: 0.2614565802327657\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.29659262841398065, Val f1: 0.5587027072906494\n",
            "Val loss: 0.2844003996123438, Val f1: 0.537499189376831\n",
            "Val loss: 0.2815053914274488, Val f1: 0.5205822587013245\n",
            "Val loss: 0.2812668288007696, Val f1: 0.5154510736465454\n",
            "Val loss: 0.27908864011198786, Val f1: 0.5156959295272827\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4086693525314331, Val f1: 0.7776681780815125\n",
            "Val loss: 0.3388035178184509, Val f1: 0.5890511870384216\n",
            "Val loss: 0.31505705788731575, Val f1: 0.5668774843215942\n",
            "Val loss: 0.30475305969064886, Val f1: 0.5520952939987183\n",
            "Val loss: 0.2979663440159389, Val f1: 0.5464088320732117\n",
            "\n",
            "starting Epoch 26\n",
            "Training...\n",
            "Train loss: 0.27680058777332306\n",
            "Train loss: 0.2647756571355073\n",
            "Train loss: 0.2634801813534328\n",
            "Train loss: 0.2607269093711325\n",
            "Train loss: 0.2581380681466248\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3033204403790561, Val f1: 0.5746200084686279\n",
            "Val loss: 0.29112640541532764, Val f1: 0.555256724357605\n",
            "Val loss: 0.2883166330201285, Val f1: 0.5469149947166443\n",
            "Val loss: 0.28709645537619893, Val f1: 0.5433452725410461\n",
            "Val loss: 0.28645677889807747, Val f1: 0.5408222675323486\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.41961655020713806, Val f1: 0.7991962432861328\n",
            "Val loss: 0.3475394189357758, Val f1: 0.6173897981643677\n",
            "Val loss: 0.3234248496592045, Val f1: 0.592113733291626\n",
            "Val loss: 0.31278434666720306, Val f1: 0.5734801292419434\n",
            "Val loss: 0.3059484724487577, Val f1: 0.5681314468383789\n",
            "\n",
            "starting Epoch 27\n",
            "Training...\n",
            "Train loss: 0.27777032825079834\n",
            "Train loss: 0.26751116954762005\n",
            "Train loss: 0.2649765248809542\n",
            "Train loss: 0.2613191804353227\n",
            "Train loss: 0.2584334162332244\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.29927128282460297, Val f1: 0.5594470500946045\n",
            "Val loss: 0.28366272345833154, Val f1: 0.5459233522415161\n",
            "Val loss: 0.28075116191591537, Val f1: 0.5379718542098999\n",
            "Val loss: 0.27964896661169986, Val f1: 0.5321416258811951\n",
            "Val loss: 0.2774737373752109, Val f1: 0.5338384509086609\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4060995578765869, Val f1: 0.8002638816833496\n",
            "Val loss: 0.3367825984954834, Val f1: 0.6083101630210876\n",
            "Val loss: 0.3133573979139328, Val f1: 0.5835838317871094\n",
            "Val loss: 0.3030715368010781, Val f1: 0.5667188763618469\n",
            "Val loss: 0.29624647966453005, Val f1: 0.5600312352180481\n",
            "\n",
            "starting Epoch 28\n",
            "Training...\n",
            "Train loss: 0.2742484347386794\n",
            "Train loss: 0.2595283486272978\n",
            "Train loss: 0.25541517308780126\n",
            "Train loss: 0.2543659616023936\n",
            "Train loss: 0.2550405483629744\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2975749210877852, Val f1: 0.5860856175422668\n",
            "Val loss: 0.2857853148294532, Val f1: 0.5527565479278564\n",
            "Val loss: 0.28156109877995084, Val f1: 0.5453276634216309\n",
            "Val loss: 0.2783573066934626, Val f1: 0.5410904288291931\n",
            "Val loss: 0.2767389522770704, Val f1: 0.5436396598815918\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.40465210378170013, Val f1: 0.7958632707595825\n",
            "Val loss: 0.3357321500778198, Val f1: 0.6141857504844666\n",
            "Val loss: 0.3123982287943363, Val f1: 0.5876992344856262\n",
            "Val loss: 0.30217723683877423, Val f1: 0.5737125873565674\n",
            "Val loss: 0.2953903057745525, Val f1: 0.566931962966919\n",
            "\n",
            "starting Epoch 29\n",
            "Training...\n",
            "Train loss: 0.2767582169987939\n",
            "Train loss: 0.26401724698750867\n",
            "Train loss: 0.25823280215263367\n",
            "Train loss: 0.2565697364984675\n",
            "Train loss: 0.25502291650085124\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2814138206568631, Val f1: 0.558751106262207\n",
            "Val loss: 0.2717088733030402, Val f1: 0.5226259827613831\n",
            "Val loss: 0.26602556152003154, Val f1: 0.5202118754386902\n",
            "Val loss: 0.2649941793147554, Val f1: 0.5130290985107422\n",
            "Val loss: 0.26476016564894533, Val f1: 0.5131011009216309\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3880455493927002, Val f1: 0.7781845927238464\n",
            "Val loss: 0.32255287766456603, Val f1: 0.581103503704071\n",
            "Val loss: 0.2999095618724823, Val f1: 0.5573244094848633\n",
            "Val loss: 0.29016460342840716, Val f1: 0.5434441566467285\n",
            "Val loss: 0.28336074841873987, Val f1: 0.5401884317398071\n",
            "\n",
            "starting Epoch 30\n",
            "Training...\n",
            "Train loss: 0.27445441619916394\n",
            "Train loss: 0.26137059603048407\n",
            "Train loss: 0.25829842346055165\n",
            "Train loss: 0.2576581147756982\n",
            "Train loss: 0.2549381248526654\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2975166819312356, Val f1: 0.5604133605957031\n",
            "Val loss: 0.2760826219683108, Val f1: 0.5504624843597412\n",
            "Val loss: 0.272373218195779, Val f1: 0.544593334197998\n",
            "Val loss: 0.2694569876853456, Val f1: 0.5443702340126038\n",
            "Val loss: 0.2680230390722469, Val f1: 0.5388236045837402\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3932238072156906, Val f1: 0.7988900542259216\n",
            "Val loss: 0.3268249988555908, Val f1: 0.6079114675521851\n",
            "Val loss: 0.30396315827965736, Val f1: 0.5825255513191223\n",
            "Val loss: 0.2940768057649786, Val f1: 0.5700268745422363\n",
            "Val loss: 0.28731632445539745, Val f1: 0.5647733211517334\n",
            "\n",
            "starting Epoch 31\n",
            "Training...\n",
            "Train loss: 0.27205687896771863\n",
            "Train loss: 0.258575105148813\n",
            "Train loss: 0.25307060352393557\n",
            "Train loss: 0.25349990516266924\n",
            "Train loss: 0.25113869622602303\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.27804090082645416, Val f1: 0.6007014513015747\n",
            "Val loss: 0.2732084063084229, Val f1: 0.5703340768814087\n",
            "Val loss: 0.27097093548093526, Val f1: 0.5607588887214661\n",
            "Val loss: 0.2702553950725718, Val f1: 0.5540135502815247\n",
            "Val loss: 0.2689696977199134, Val f1: 0.5499181747436523\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3941154256463051, Val f1: 0.8134351968765259\n",
            "Val loss: 0.3276156634092331, Val f1: 0.6173914074897766\n",
            "Val loss: 0.3047188613563776, Val f1: 0.5932849645614624\n",
            "Val loss: 0.29475331983783026, Val f1: 0.5808939933776855\n",
            "Val loss: 0.28797438953604015, Val f1: 0.5741757750511169\n",
            "\n",
            "starting Epoch 32\n",
            "Training...\n",
            "Train loss: 0.27121817794713104\n",
            "Train loss: 0.261654077016789\n",
            "Train loss: 0.25548726873738425\n",
            "Train loss: 0.25007840769088013\n",
            "Train loss: 0.2508167538602473\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2867864180694927, Val f1: 0.6056779623031616\n",
            "Val loss: 0.2723401746024256, Val f1: 0.5733841061592102\n",
            "Val loss: 0.268350373847144, Val f1: 0.5599523782730103\n",
            "Val loss: 0.2669960818392165, Val f1: 0.5533880591392517\n",
            "Val loss: 0.2665053797980486, Val f1: 0.5499274730682373\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3909682407975197, Val f1: 0.8187936544418335\n",
            "Val loss: 0.3248726218938828, Val f1: 0.6237092018127441\n",
            "Val loss: 0.3020393270999193, Val f1: 0.5972791910171509\n",
            "Val loss: 0.2921613251621073, Val f1: 0.5840228199958801\n",
            "Val loss: 0.2853892466851643, Val f1: 0.5775830149650574\n",
            "\n",
            "starting Epoch 33\n",
            "Training...\n",
            "Train loss: 0.2744634219191291\n",
            "Train loss: 0.25963902667812677\n",
            "Train loss: 0.25444511004856657\n",
            "Train loss: 0.25241062520666324\n",
            "Train loss: 0.25144235208883126\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2867006632414731, Val f1: 0.6122024655342102\n",
            "Val loss: 0.27836744033772015, Val f1: 0.5815995335578918\n",
            "Val loss: 0.2728829392365047, Val f1: 0.571596086025238\n",
            "Val loss: 0.2708288630906572, Val f1: 0.5647220611572266\n",
            "Val loss: 0.26913597548412066, Val f1: 0.5614427328109741\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3955352008342743, Val f1: 0.8130507469177246\n",
            "Val loss: 0.32864423394203185, Val f1: 0.6356650590896606\n",
            "Val loss: 0.30562057346105576, Val f1: 0.6067312955856323\n",
            "Val loss: 0.29565798152576794, Val f1: 0.5925400853157043\n",
            "Val loss: 0.28889779320784975, Val f1: 0.5876986384391785\n",
            "\n",
            "starting Epoch 34\n",
            "Training...\n",
            "Train loss: 0.26871995492414996\n",
            "Train loss: 0.25732394664183905\n",
            "Train loss: 0.2521895651306425\n",
            "Train loss: 0.25157598865793107\n",
            "Train loss: 0.2500405887425956\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.29684199528260663, Val f1: 0.6169070601463318\n",
            "Val loss: 0.28310905850451923, Val f1: 0.5908005237579346\n",
            "Val loss: 0.27945827075413293, Val f1: 0.580014705657959\n",
            "Val loss: 0.276094396063622, Val f1: 0.575520932674408\n",
            "Val loss: 0.2745756395792557, Val f1: 0.573089599609375\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4035566747188568, Val f1: 0.8395935297012329\n",
            "Val loss: 0.3349001705646515, Val f1: 0.6532711386680603\n",
            "Val loss: 0.3115806579589844, Val f1: 0.6220747232437134\n",
            "Val loss: 0.3014125336300243, Val f1: 0.6043216586112976\n",
            "Val loss: 0.2946223999772753, Val f1: 0.5996869802474976\n",
            "\n",
            "starting Epoch 35\n",
            "Training...\n",
            "Train loss: 0.2663344551216472\n",
            "Train loss: 0.25653378924597864\n",
            "Train loss: 0.2517589713845934\n",
            "Train loss: 0.24958235437565662\n",
            "Train loss: 0.24833958002470308\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.30009427124803717, Val f1: 0.6099143028259277\n",
            "Val loss: 0.2838635606610257, Val f1: 0.5911056399345398\n",
            "Val loss: 0.27731378589357647, Val f1: 0.5891876816749573\n",
            "Val loss: 0.275715985830794, Val f1: 0.5837566256523132\n",
            "Val loss: 0.27514634041462915, Val f1: 0.5803098082542419\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.404448539018631, Val f1: 0.8322409391403198\n",
            "Val loss: 0.3356723427772522, Val f1: 0.6543294787406921\n",
            "Val loss: 0.31233036145567894, Val f1: 0.6229984760284424\n",
            "Val loss: 0.302141162482175, Val f1: 0.6084966659545898\n",
            "Val loss: 0.29539952533585684, Val f1: 0.602630078792572\n",
            "\n",
            "starting Epoch 36\n",
            "Training...\n",
            "Train loss: 0.2628138309175318\n",
            "Train loss: 0.25206783089948737\n",
            "Train loss: 0.24972816790853228\n",
            "Train loss: 0.24660708232128875\n",
            "Train loss: 0.2459556169934192\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2832195907831192, Val f1: 0.6282184720039368\n",
            "Val loss: 0.27237096493658813, Val f1: 0.6041268706321716\n",
            "Val loss: 0.27088355209146225, Val f1: 0.5932755470275879\n",
            "Val loss: 0.2719189204434131, Val f1: 0.5823537707328796\n",
            "Val loss: 0.2708106518296872, Val f1: 0.5816131830215454\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.39876456558704376, Val f1: 0.844070553779602\n",
            "Val loss: 0.33096049427986146, Val f1: 0.6604057550430298\n",
            "Val loss: 0.3078467659652233, Val f1: 0.6256204843521118\n",
            "Val loss: 0.29781254855069245, Val f1: 0.6100161671638489\n",
            "Val loss: 0.2910969299929483, Val f1: 0.6044318675994873\n",
            "\n",
            "starting Epoch 37\n",
            "Training...\n",
            "Train loss: 0.2636832974173806\n",
            "Train loss: 0.2521431957897933\n",
            "Train loss: 0.25059738499777656\n",
            "Train loss: 0.24668701183288655\n",
            "Train loss: 0.24581232894275148\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.28633636236190796, Val f1: 0.6155257821083069\n",
            "Val loss: 0.26951688333697943, Val f1: 0.5846642851829529\n",
            "Val loss: 0.2665930062532425, Val f1: 0.571579098701477\n",
            "Val loss: 0.26294959130439355, Val f1: 0.5749562382698059\n",
            "Val loss: 0.261258476113869, Val f1: 0.573997974395752\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.38567038625478745, Val f1: 0.8397872447967529\n",
            "Val loss: 0.3202816635370255, Val f1: 0.6467728614807129\n",
            "Val loss: 0.2976124305278063, Val f1: 0.6158987283706665\n",
            "Val loss: 0.2879350876266306, Val f1: 0.6003209352493286\n",
            "Val loss: 0.2812374532222748, Val f1: 0.5966036319732666\n",
            "\n",
            "starting Epoch 38\n",
            "Training...\n",
            "Train loss: 0.26702939651229163\n",
            "Train loss: 0.25156299899453705\n",
            "Train loss: 0.2492075434752873\n",
            "Train loss: 0.24682773617987938\n",
            "Train loss: 0.24588194918834558\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2828368910334327, Val f1: 0.6407621502876282\n",
            "Val loss: 0.27044647672901984, Val f1: 0.6036320924758911\n",
            "Val loss: 0.2694671158279691, Val f1: 0.5914157629013062\n",
            "Val loss: 0.26843268059669656, Val f1: 0.5861414074897766\n",
            "Val loss: 0.26747349687552047, Val f1: 0.5791649222373962\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3936602771282196, Val f1: 0.8348658084869385\n",
            "Val loss: 0.3266254007816315, Val f1: 0.662387490272522\n",
            "Val loss: 0.3036040849983692, Val f1: 0.6290829181671143\n",
            "Val loss: 0.2937898500399156, Val f1: 0.6137421131134033\n",
            "Val loss: 0.28710745700768064, Val f1: 0.6081879734992981\n",
            "\n",
            "starting Epoch 39\n",
            "Training...\n",
            "Train loss: 0.2632045502012426\n",
            "Train loss: 0.25258536960767664\n",
            "Train loss: 0.24873755616801127\n",
            "Train loss: 0.2477806178813285\n",
            "Train loss: 0.2448360692646544\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2793721773407676, Val f1: 0.6329418420791626\n",
            "Val loss: 0.27156056139780127, Val f1: 0.5934039354324341\n",
            "Val loss: 0.26682516080992563, Val f1: 0.5900592803955078\n",
            "Val loss: 0.26551561659954964, Val f1: 0.5863028168678284\n",
            "Val loss: 0.2651346179388337, Val f1: 0.5803574323654175\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3904893696308136, Val f1: 0.8338621854782104\n",
            "Val loss: 0.3240643501281738, Val f1: 0.6600662469863892\n",
            "Val loss: 0.30124005302786827, Val f1: 0.6272979974746704\n",
            "Val loss: 0.29151511734182184, Val f1: 0.6110552549362183\n",
            "Val loss: 0.28479800479752676, Val f1: 0.6049928069114685\n",
            "\n",
            "starting Epoch 40\n",
            "Training...\n",
            "Train loss: 0.26276898790489545\n",
            "Train loss: 0.25013918980308203\n",
            "Train loss: 0.2452317944594792\n",
            "Train loss: 0.24309872154225695\n",
            "Train loss: 0.24285604842638564\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2906767590479417, Val f1: 0.6236394643783569\n",
            "Val loss: 0.27743693862272345, Val f1: 0.5979750752449036\n",
            "Val loss: 0.27192565373011995, Val f1: 0.5963817238807678\n",
            "Val loss: 0.2707337179082505, Val f1: 0.5943717956542969\n",
            "Val loss: 0.2690550988003359, Val f1: 0.5891901254653931\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.39596469700336456, Val f1: 0.8419675230979919\n",
            "Val loss: 0.32852686643600465, Val f1: 0.665135383605957\n",
            "Val loss: 0.3055356815457344, Val f1: 0.6318076252937317\n",
            "Val loss: 0.2956469980153171, Val f1: 0.6200065612792969\n",
            "Val loss: 0.28893303338970455, Val f1: 0.6148985624313354\n",
            "\n",
            "starting Epoch 41\n",
            "Training...\n",
            "Train loss: 0.26699867709116504\n",
            "Train loss: 0.25119944435098895\n",
            "Train loss: 0.2483512350491115\n",
            "Train loss: 0.24521164754603772\n",
            "Train loss: 0.2430622006876994\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.28378294814716687, Val f1: 0.6289759874343872\n",
            "Val loss: 0.2692599024461663, Val f1: 0.6007372140884399\n",
            "Val loss: 0.26537843942642214, Val f1: 0.5953006148338318\n",
            "Val loss: 0.26363373024666564, Val f1: 0.5924578905105591\n",
            "Val loss: 0.2628518565226409, Val f1: 0.5853735208511353\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.38631555438041687, Val f1: 0.8282829523086548\n",
            "Val loss: 0.32073577046394347, Val f1: 0.6599828004837036\n",
            "Val loss: 0.2983030490577221, Val f1: 0.6283392906188965\n",
            "Val loss: 0.2885837663303722, Val f1: 0.6137911081314087\n",
            "Val loss: 0.2818395474127361, Val f1: 0.61069655418396\n",
            "\n",
            "starting Epoch 42\n",
            "Training...\n",
            "Train loss: 0.26525781506841833\n",
            "Train loss: 0.25153810044993524\n",
            "Train loss: 0.24457052903515952\n",
            "Train loss: 0.24373045912448396\n",
            "Train loss: 0.24287095489138263\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.27905281159010803, Val f1: 0.6174051761627197\n",
            "Val loss: 0.2664462289084559, Val f1: 0.6025819182395935\n",
            "Val loss: 0.26562572887965613, Val f1: 0.5959242582321167\n",
            "Val loss: 0.26405844409415064, Val f1: 0.5917453765869141\n",
            "Val loss: 0.26284554954302514, Val f1: 0.5889352560043335\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3876792937517166, Val f1: 0.8279527425765991\n",
            "Val loss: 0.32184571623802183, Val f1: 0.66218501329422\n",
            "Val loss: 0.29923543706536293, Val f1: 0.6308051347732544\n",
            "Val loss: 0.2895412959835746, Val f1: 0.6161757707595825\n",
            "Val loss: 0.2828562376754625, Val f1: 0.6116665005683899\n",
            "\n",
            "starting Epoch 43\n",
            "Training...\n",
            "Train loss: 0.2590741921554912\n",
            "Train loss: 0.2491543040327404\n",
            "Train loss: 0.24731028165136065\n",
            "Train loss: 0.24487044519566475\n",
            "Train loss: 0.24282166589114626\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2828329422257163, Val f1: 0.6214426755905151\n",
            "Val loss: 0.2720903359029604, Val f1: 0.5970990061759949\n",
            "Val loss: 0.2673168365444456, Val f1: 0.5926560759544373\n",
            "Val loss: 0.2644701321074303, Val f1: 0.5924505591392517\n",
            "Val loss: 0.2639773791119204, Val f1: 0.5932353138923645\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3894018679857254, Val f1: 0.857646107673645\n",
            "Val loss: 0.3231657087802887, Val f1: 0.673006534576416\n",
            "Val loss: 0.30056747049093246, Val f1: 0.6390368938446045\n",
            "Val loss: 0.2908394255421378, Val f1: 0.6263909339904785\n",
            "Val loss: 0.2841602201972689, Val f1: 0.620661735534668\n",
            "\n",
            "starting Epoch 44\n",
            "Training...\n",
            "Train loss: 0.26346385749903595\n",
            "Train loss: 0.2493246035731357\n",
            "Train loss: 0.244583152447428\n",
            "Train loss: 0.24285525590815443\n",
            "Train loss: 0.24270416240570908\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2732526646419005, Val f1: 0.6703250408172607\n",
            "Val loss: 0.26648204222969385, Val f1: 0.6255992650985718\n",
            "Val loss: 0.265441186938967, Val f1: 0.6072456240653992\n",
            "Val loss: 0.26409316062927246, Val f1: 0.5975551605224609\n",
            "Val loss: 0.26215874239549797, Val f1: 0.5947161316871643\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.38717785477638245, Val f1: 0.848949670791626\n",
            "Val loss: 0.32126967906951903, Val f1: 0.6710706353187561\n",
            "Val loss: 0.29878492653369904, Val f1: 0.6377888321876526\n",
            "Val loss: 0.2890979051589966, Val f1: 0.6241050958633423\n",
            "Val loss: 0.2823972574302128, Val f1: 0.6182866096496582\n",
            "\n",
            "starting Epoch 45\n",
            "Training...\n",
            "Train loss: 0.2649899653413079\n",
            "Train loss: 0.2506380301454793\n",
            "Train loss: 0.24476880048002517\n",
            "Train loss: 0.2424182080207987\n",
            "Train loss: 0.2414054936271603\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.28856854953549127, Val f1: 0.6510095596313477\n",
            "Val loss: 0.2742214656394461, Val f1: 0.6164136528968811\n",
            "Val loss: 0.2709948292800358, Val f1: 0.6081516146659851\n",
            "Val loss: 0.26946349632232747, Val f1: 0.603147566318512\n",
            "Val loss: 0.2675120431487843, Val f1: 0.6010398864746094\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.39466896653175354, Val f1: 0.8569194078445435\n",
            "Val loss: 0.327246767282486, Val f1: 0.6740524172782898\n",
            "Val loss: 0.30446934327483177, Val f1: 0.640079140663147\n",
            "Val loss: 0.2947271032766862, Val f1: 0.6281947493553162\n",
            "Val loss: 0.28799846449068617, Val f1: 0.6221439242362976\n",
            "\n",
            "starting Epoch 46\n",
            "Training...\n",
            "Train loss: 0.26016223430633545\n",
            "Train loss: 0.24929757092310034\n",
            "Train loss: 0.2456080836909158\n",
            "Train loss: 0.24184140277669786\n",
            "Train loss: 0.2412560592263432\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.28644213351336395, Val f1: 0.6599903702735901\n",
            "Val loss: 0.2763859124287315, Val f1: 0.6221071481704712\n",
            "Val loss: 0.2721458469118391, Val f1: 0.6143439412117004\n",
            "Val loss: 0.26934903891796763, Val f1: 0.612176239490509\n",
            "Val loss: 0.2689015231395172, Val f1: 0.6039643287658691\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.39675796777009964, Val f1: 0.8792674541473389\n",
            "Val loss: 0.3289229780435562, Val f1: 0.6842431426048279\n",
            "Val loss: 0.30610361509025097, Val f1: 0.6469755172729492\n",
            "Val loss: 0.29631309482184326, Val f1: 0.6329607963562012\n",
            "Val loss: 0.28954384263072697, Val f1: 0.6270832419395447\n",
            "\n",
            "starting Epoch 47\n",
            "Training...\n",
            "Train loss: 0.25026009705933655\n",
            "Train loss: 0.24244166910648346\n",
            "Train loss: 0.24017101100512914\n",
            "Train loss: 0.24052313160389027\n",
            "Train loss: 0.23983939977015478\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.27359541031447326, Val f1: 0.6495450139045715\n",
            "Val loss: 0.26599585297315015, Val f1: 0.6273506879806519\n",
            "Val loss: 0.2647602451699121, Val f1: 0.6109121441841125\n",
            "Val loss: 0.26367923997818155, Val f1: 0.6065788865089417\n",
            "Val loss: 0.26288665704807995, Val f1: 0.604035496711731\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.38848812878131866, Val f1: 0.8664567470550537\n",
            "Val loss: 0.3221931278705597, Val f1: 0.6824975609779358\n",
            "Val loss: 0.2998712658882141, Val f1: 0.6435585021972656\n",
            "Val loss: 0.29025389931418677, Val f1: 0.6307070255279541\n",
            "Val loss: 0.28354773989745546, Val f1: 0.6254294514656067\n",
            "\n",
            "starting Epoch 48\n",
            "Training...\n",
            "Train loss: 0.25736443292010913\n",
            "Train loss: 0.24565935653188956\n",
            "Train loss: 0.2452793504510607\n",
            "Train loss: 0.2423552835875369\n",
            "Train loss: 0.24096171027522975\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2878869121724909, Val f1: 0.6554283499717712\n",
            "Val loss: 0.2753327359323916, Val f1: 0.6233789324760437\n",
            "Val loss: 0.2710621382508959, Val f1: 0.6148128509521484\n",
            "Val loss: 0.26938485338332807, Val f1: 0.6135838031768799\n",
            "Val loss: 0.26763238189584115, Val f1: 0.6095055937767029\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3946065604686737, Val f1: 0.8818415403366089\n",
            "Val loss: 0.32711921334266664, Val f1: 0.6877590417861938\n",
            "Val loss: 0.3047780767083168, Val f1: 0.6500951051712036\n",
            "Val loss: 0.29496828534386377, Val f1: 0.6355037689208984\n",
            "Val loss: 0.2881532460451126, Val f1: 0.6304085850715637\n",
            "\n",
            "starting Epoch 49\n",
            "Training...\n",
            "Train loss: 0.2571395364674655\n",
            "Train loss: 0.24571777167527573\n",
            "Train loss: 0.2414566159248352\n",
            "Train loss: 0.2397593855857849\n",
            "Train loss: 0.23927896457203365\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.27966269037940283, Val f1: 0.6553681492805481\n",
            "Val loss: 0.2681836881067442, Val f1: 0.625144898891449\n",
            "Val loss: 0.2638233461550304, Val f1: 0.6172352433204651\n",
            "Val loss: 0.2617476395470031, Val f1: 0.6129328608512878\n",
            "Val loss: 0.26097353080571706, Val f1: 0.6056262254714966\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.38506587594747543, Val f1: 0.8622333407402039\n",
            "Val loss: 0.31942433416843413, Val f1: 0.67850661277771\n",
            "Val loss: 0.297475116327405, Val f1: 0.6445002555847168\n",
            "Val loss: 0.2879133400591937, Val f1: 0.6313222050666809\n",
            "Val loss: 0.2811623439192772, Val f1: 0.6266044974327087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "eH7ws8dXmEn5",
        "outputId": "3e742ebc-e57f-4d67-eb70-36240964a9d0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "q32B6i6BmGOc",
        "outputId": "9c885f61-e361-4622-d564-001b386fbcbb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj0-XaCkYz8w"
      },
      "source": [
        "Для анализа ошибок можно посмотреть на те примеры, которые мы (не)правильно предсказываем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHtAIt5eCrP5"
      },
      "source": [
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте \n",
        "            for pred, gold, text in zip(preds, ys, texts):\n",
        "              text = ''.join([id2symbol[int(symbol)] for symbol in text if symbol !=0])\n",
        "              if round(pred.item()) > gold:\n",
        "                fp.append(text)\n",
        "              elif round(pred.item()) < gold:\n",
        "                fn.append(text)\n",
        "              elif round(pred.item()) == gold == 1:\n",
        "                tp.append(text)\n",
        "              elif round(pred.item()) == gold == 0:\n",
        "                tn.append(text)\n",
        "    return fp, fn, tp, tn"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phc1BAE4MKhU"
      },
      "source": [
        "fp, fn, tp, tn = predict(model, val_iterator)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFiLocEC6dUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9c6572-ae90-4cbb-e767-b1485882fa2e"
      },
      "source": [
        "print('что правильно предсказываем:', tp[:100])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "что правильно предсказываем: ['ГОЛЯМ', 'Андреевой', 'Лама', 'Калнауз', 'Сэндберг', 'Бакалинский', 'Франсом', 'Маркиным', 'ФАЙКОВОЙ', 'Сараджевой', 'Желобаева', 'Биат', 'Гречаниновнам', 'Лазарев', 'Холмс', 'Корчагин', 'Смик', 'Локвуде', 'Гришановым', 'Басков', 'Бартоны', 'Коростелев', 'Волынского', 'Вертковы', 'АБМАЙКИНА', 'Рябченко', 'Бафой', 'Богдановского', 'Бархударов', 'ШИМАНОВЫЙ', 'Бэром', 'Кричковська', 'Фролов', 'Хефнер', 'Гнеденко', 'Тимуркаевичах', 'Гебель', 'Ривас', 'Шкрябина', 'Завьялова', 'Ариасом', 'Лужина', 'Хофман', 'Булычев', 'Касперский', 'Рагинский', 'Комаровой', 'ЭБЕР', 'Башмачкиных', 'Гослинг', 'Мышковой', 'Кокачев', 'Ломова', 'Гайдер', 'Губанов', 'Линдеманах', 'Бараева', 'Буниной', 'Скоробагатый', 'Лебедев', 'Шкидченко', 'Фельдмана', 'Гирина', 'Лейка', 'Абдикалиевый', 'Бостону', 'Штайнера', 'Януковичу', 'Школенко', 'Малкина', 'Керимов', 'КАУКИН', 'Куйбышевских', 'Кайсаровым', 'Баха', 'Мендельсону', 'Калинина', 'Бойкову', 'Ломоносове', 'Шейфер', 'Гормана', 'Брене', 'Косяков', 'Гендель', 'Моряков', 'Выгодская', 'Бильманам', 'Шкрабалюк', 'Безбородко', 'Фейербахе', 'Серов', 'Голубы', 'Бабичевым', 'Лавровского', 'Журавлеву', 'Шарибове', 'Янгов', 'Сычеву', 'Гаскойном', 'Лоренс']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeBGHMjm4LBJ",
        "outputId": "455b8bda-35c1-4d09-b4f7-117091e7a1b2"
      },
      "source": [
        "print('ошибочно не относим к фамилиям:', fn[:100])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно не относим к фамилиям: ['Турки', 'Саммерса', 'КИРКА', 'Менделю', 'Кавендише', 'ИЛЬЧУК', 'Козодои', 'ПЛАНТ', 'ИВКА', 'ЧУПРИКОВА', 'Рае', 'риска', 'старт', 'Виде', 'АЛИСЕЕНКО', 'такер', 'ИНДЮШКИНАХ', 'Лацисом', 'ПЛЮТО', 'суярок', 'КОЛМОГОРОВА', 'кура', 'КИРИЛЕНКА', 'Локхартом', 'РОШ', 'КУЗНЕЦОВА', 'Твист', 'Рущаке', 'Кафтить', 'Сагрять', 'МАМБЕТЖАНОВ', 'Пога', 'ЯНУКОВИЧ', 'ОНЬКОВЫ', 'победители', 'САРИТОВЫЙ', 'Петерсону', 'АБУМ', 'Уруспаи', 'АГАГЮЛОВАМИ', 'Дурицыным', 'болван', 'ЧЕРЕДНИКОВ', 'алшоразовны', 'провалы', 'Остапенко', 'Дериземля', 'Ярмышеве', 'Кетрушенной', 'СЕНГЕРБАЕВНЫ', 'Мафедзовами', 'ДЖУСИПОВ', 'Стайне', 'Спарксе', 'Стокса', 'Дейку', 'ПОСКРЯКОВЫЙ', 'техника', 'Тхорёвая', 'врач', 'Давидкевичей', 'ВОЛЬЦМАНУ', 'мухамбеткалиев', 'Спрингер', 'Давлетхан', 'тезиковой', 'Ошана', 'МОНРО', 'Армстронгу', 'Оуэном', 'совет', 'Рихтером', 'Ищенко', 'СОТАЕВА', 'Абдулкадыр', 'Ливингстоне', 'Берию', 'современники', 'ДЗЬ', 'Пратасень', 'ГОВОРКОВЫХ', 'РОМАНКО', 'Михеля', 'генералы', 'убер', 'Истюфей', 'КЛЕЙМЕНОВ', 'Морленда', 'старковского', 'турганжанам', 'придаток', 'тленчии', 'художник', 'Скавронський', 'Серджент', 'Андрусом', 'Рябеках', 'мильштейн', 'Капице', 'Гилмором']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztrjLRu34e-B",
        "outputId": "8f86977e-ff06-4552-a023-ab55c98974b1"
      },
      "source": [
        "print('ошибочно считаем фамилиями:', fp[:100])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно считаем фамилиями: ['Колючек', 'Комарье', 'Камберленд', 'Каббалы', 'Раздевалка', 'Чечней', 'Зону', 'Гипюр', 'обычаев', 'Гроза', 'Тауэра', 'Клоуном', 'Волдыре', 'КИР', 'Линкольне', 'Васильевне', 'Гротон', 'Сэмсон', 'Ванну', 'Капернаум', 'Борису', 'Кивер', 'Самоварчик', 'Теологов', 'Роном', 'Аквариуму', 'Пектин', 'Шоферу', 'Грифель', 'Млрд', 'Человекам', 'Габоном', 'Валентиновича', 'Дней', 'Глашкой', 'СУБД', 'Рэма', 'Коуча', 'Лаша', 'Коронера', 'Биллингсе', 'Блудников', 'Чимборасо', 'РАНЧО', 'Ваза', 'Формулой', 'БАХРЕЙН', 'Серена', 'Мохамед', 'Гротона', 'Алабама', 'Фивы', 'Кочаны', 'Гисборна', 'Букраннерами', 'Халлибертона', 'Кухня', 'Комнатенка', 'ВИНА', 'Концерна', 'Разка', 'ЛИНДЕ', 'Браво', 'Хеврона', 'Аду', 'Клопов', 'Азербайджанцев', 'Арслан', 'Креп', 'Хишама', 'Линчберг', 'Бечевки', 'Филипповны', 'Государь', 'Айшу', 'Билла', 'Гонконге', 'Чтива', 'Челекен', 'Жаков', 'Хэллоуин', 'балдахинов', 'Рим', 'Арнольдович', 'Траву', 'Шабатом', 'Корнелиусу', 'Катангу', 'ТЯГАЧ', 'Парча', 'Абдуловна', 'Маркизу', 'Ручеек', 'Батат', 'ВХОДЫ', 'Рувим', 'Грудь', 'Денечек', 'Джинсах', 'Кирквудом']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZL6OlYcbE9d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}